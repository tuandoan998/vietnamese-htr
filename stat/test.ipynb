{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "À 0041 0300\n",
      "Á 0041 0301\n",
      "Ả 0041 0309\n",
      "Ã 0041 0303\n",
      "Ạ 0041 0323\n",
      "Â 0041 0302\n",
      "Ầ 00C2 0300\n",
      "Ấ 00C2 0301\n",
      "Ẩ 00C2 0309\n",
      "Ẫ 00C2 0303\n",
      "Ậ 1EA0 0302\n",
      "Ă 0041 0306\n",
      "Ằ 0102 0300\n",
      "Ắ 0102 0301\n",
      "Ẳ 0102 0309\n",
      "Ẵ 0102 0303\n",
      "Ặ 1EA0 0306\n",
      "à 0061 0300\n",
      "á 0061 0301\n",
      "ả 0061 0309\n",
      "ã 0061 0303\n",
      "ạ 0061 0323\n",
      "â 0061 0302\n",
      "ầ 00E2 0300\n",
      "ấ 00E2 0301\n",
      "ẩ 00E2 0309\n",
      "ẫ 00E2 0303\n",
      "ậ 1EA1 0302\n",
      "ă 0061 0306\n",
      "ằ 0103 0300\n",
      "ắ 0103 0301\n",
      "ẳ 0103 0309\n",
      "ẵ 0103 0303\n",
      "ặ 1EA1 0306\n",
      "È 0045 0300\n",
      "É 0045 0301\n",
      "Ẻ 0045 0309\n",
      "Ẽ 0045 0303\n",
      "Ẹ 0045 0323\n",
      "Ê 0045 0302\n",
      "Ề 00CA 0300\n",
      "Ế 00CA 0301\n",
      "Ể 00CA 0309\n",
      "Ễ 00CA 0303\n",
      "Ệ 1EB8 0302\n",
      "è 0065 0300\n",
      "é 0065 0301\n",
      "ẻ 0065 0309\n",
      "ẽ 0065 0303\n",
      "ẹ 0065 0323\n",
      "ê 0065 0302\n",
      "ề 00EA 0300\n",
      "ế 00EA 0301\n",
      "ể 00EA 0309\n",
      "ễ 00EA 0303\n",
      "ệ 1EB9 0302\n",
      "đ \n",
      "Ì 0049 0300\n",
      "Í 0049 0301\n",
      "Ỉ 0049 0309\n",
      "Ĩ 0049 0303\n",
      "Ị 0049 0323\n",
      "ì 0069 0300\n",
      "í 0069 0301\n",
      "ỉ 0069 0309\n",
      "ĩ 0069 0303\n",
      "ị 0069 0323\n",
      "Ò 004F 0300\n",
      "Ó 004F 0301\n",
      "Ỏ 004F 0309\n",
      "Õ 004F 0303\n",
      "Ọ 004F 0323\n",
      "Ô 004F 0302\n",
      "Ồ 00D4 0300\n",
      "Ố 00D4 0301\n",
      "Ổ 00D4 0309\n",
      "Ỗ 00D4 0303\n",
      "Ộ 1ECC 0302\n",
      "Ơ 004F 031B\n",
      "Ờ 01A0 0300\n",
      "Ớ 01A0 0301\n",
      "Ở 01A0 0309\n",
      "Ỡ 01A0 0303\n",
      "Ợ 01A0 0323\n",
      "ò 006F 0300\n",
      "ó 006F 0301\n",
      "ỏ 006F 0309\n",
      "õ 006F 0303\n",
      "ọ 006F 0323\n",
      "ô 006F 0302\n",
      "ồ 00F4 0300\n",
      "ố 00F4 0301\n",
      "ổ 00F4 0309\n",
      "ỗ 00F4 0303\n",
      "ộ 1ECD 0302\n",
      "ơ 006F 031B\n",
      "ờ 01A1 0300\n",
      "ớ 01A1 0301\n",
      "ở 01A1 0309\n",
      "ỡ 01A1 0303\n",
      "ợ 01A1 0323\n",
      "Ù 0055 0300\n",
      "Ú 0055 0301\n",
      "Ủ 0055 0309\n",
      "Ũ 0055 0303\n",
      "Ụ 0055 0323\n",
      "Ư 0055 031B\n",
      "Ừ 01AF 0300\n",
      "Ứ 01AF 0301\n",
      "Ử 01AF 0309\n",
      "Ữ 01AF 0303\n",
      "Ự 01AF 0323\n",
      "ù 0075 0300\n",
      "ú 0075 0301\n",
      "ủ 0075 0309\n",
      "ũ 0075 0303\n",
      "ụ 0075 0323\n",
      "ư 0075 031B\n",
      "ừ 01B0 0300\n",
      "ứ 01B0 0301\n",
      "ử 01B0 0309\n",
      "ữ 01B0 0303\n",
      "ự 01B0 0323\n",
      "Ỳ 0059 0300\n",
      "Ý 0059 0301\n",
      "Ỷ 0059 0309\n",
      "Ỹ 0059 0303\n",
      "Ỵ 0059 0323\n",
      "ỳ 0079 0300\n",
      "ý 0079 0301\n",
      "ỷ 0079 0309\n",
      "ỹ 0079 0303\n",
      "ỵ 0079 0323\n"
     ]
    }
   ],
   "source": [
    "accent_letters = 'À Á Ả Ã Ạ Â Ầ Ấ Ẩ Ẫ Ậ Ă Ằ Ắ Ẳ Ẵ Ặ à á ả ã ạ â ầ ấ ẩ ẫ ậ ă ằ ắ ẳ ẵ ặ\\\n",
    "            È É Ẻ Ẽ Ẹ Ê Ề Ế Ể Ễ Ệ è é ẻ ẽ ẹ ê ề ế ể ễ ệ\\\n",
    "            đ\\\n",
    "            Ì Í Ỉ Ĩ Ị ì í ỉ ĩ ị\\\n",
    "            Ò Ó Ỏ Õ Ọ Ô Ồ Ố Ổ Ỗ Ộ Ơ Ờ Ớ Ở Ỡ Ợ ò ó ỏ õ ọ ô ồ ố ổ ỗ ộ ơ ờ ớ ở ỡ ợ\\\n",
    "            Ù Ú Ủ Ũ Ụ Ư Ừ Ứ Ử Ữ Ự ù ú ủ ũ ụ ư ừ ứ ử ữ ự\\\n",
    "            Ỳ Ý Ỷ Ỹ Ỵ ỳ ý ỷ ỹ ỵ'\n",
    "\n",
    "for letter in accent_letters.split():\n",
    "    print(letter, unicodedata.decomposition(letter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'đ'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFC', 'đ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flattening():\n",
    "    '''\n",
    "    f_type: flatening type\n",
    "        + type_1: ă, â, ê, ...\n",
    "        + type_2: type_1 + {đ, ơ, ư}\n",
    "    '''\n",
    "    def __init__(self, f_type='type_1'):\n",
    "        self.f_type = f_type\n",
    "        self.accent2unicode = {'<6>': '\\u0302', '<7>': '\\u031B', '<8>': '\\u0306', '<F>': '\\u0300', \\\n",
    "                               '<S>': '\\u0301', '<R>': '\\u0309', '<X>': '\\u0303', '<J>': '\\u0323'}\n",
    "        self.circumflex_unicodes = ['00C2', '00E2', '00CA', '00EA', '00D4', '00F4'] # â, Â, Ê, ...\n",
    "        self.breve_unicodes = ['0102', '0103'] # ă, Ă\n",
    "        self.underdot_unicodes = ['1EA0', '1EA1', '1EB8', '1EB9', '1ECC', '1ECD']\n",
    "        self._7_unicodes = ['01A0', '01A1', '01AF', '01B0']\n",
    "        self.accent_letters = 'À Á Ả Ã Ạ Â Ầ Ấ Ẩ Ẫ Ậ Ă Ằ Ắ Ẳ Ẵ Ặ à á ả ã ạ â ầ ấ ẩ ẫ ậ ă ằ ắ ẳ ẵ ặ\\\n",
    "        È É Ẻ Ẽ Ẹ Ê Ề Ế Ể Ễ Ệ è é ẻ ẽ ẹ ê ề ế ể ễ ệ\\\n",
    "        Ì Í Ỉ Ĩ Ị ì í ỉ ĩ ị\\\n",
    "        Ò Ó Ỏ Õ Ọ Ô Ồ Ố Ổ Ỗ Ộ Ơ Ờ Ớ Ở Ỡ Ợ ò ó ỏ õ ọ ô ồ ố ổ ỗ ộ ơ ờ ớ ở ỡ ợ\\\n",
    "        Ù Ú Ủ Ũ Ụ Ư Ừ Ứ Ử Ữ Ự ù ú ủ ũ ụ ư ừ ứ ử ữ ự\\\n",
    "        Ỳ Ý Ỷ Ỹ Ỵ ỳ ý ỷ ỹ ỵ'\n",
    "        if self.f_type=='type_2':\n",
    "            self.accent_letters += ' đ Đ'\n",
    "        self.accent_letters = self.accent_letters.split()\n",
    "    \n",
    "    def get_unaccent(self, letter):\n",
    "        letter = letter.encode('utf-8').decode('utf-8')\n",
    "        if self.f_type == 'type_1':\n",
    "            letter = re.sub(u'[àáảãạâầấẩẫậăằắẳẵặ]', 'a', letter)\n",
    "            letter = re.sub(u'[ÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶ]', 'A', letter)\n",
    "            letter = re.sub(u'[èéẹẻẽêềếệểễ]', 'e', letter)\n",
    "            letter = re.sub(u'[ÈÉẸẺẼÊỀẾỆỂỄ]', 'E', letter)\n",
    "            letter = re.sub(u'[òóọỏõôồốộổỗ]', 'o', letter)\n",
    "            letter = re.sub(u'[ÒÓỌỎÕÔỒỐỘỔỖ]', 'O', letter)\n",
    "            letter = re.sub(u'[ơờớợởỡ]', 'ơ', letter)\n",
    "            letter = re.sub(u'[ƠỜỚỢỞỠ]', 'Ơ', letter)\n",
    "            letter = re.sub(u'[ìíịỉĩ]', 'i', letter)\n",
    "            letter = re.sub(u'[ÌÍỊỈĨ]', 'I', letter)\n",
    "            letter = re.sub(u'[ùúụủũ]', 'u', letter)\n",
    "            letter = re.sub(u'[ÙÚỤỦŨ]', 'U', letter)\n",
    "            letter = re.sub(u'[ưừứựửữ]', 'ư', letter)\n",
    "            letter = re.sub(u'[ƯỪỨỰỬỮ]', 'Ư', letter)\n",
    "            letter = re.sub(u'[ỳýỵỷỹ]', 'y', letter)\n",
    "            letter = re.sub(u'[ỲÝỴỶỸ]', 'Y', letter)\n",
    "            return letter\n",
    "        elif self.f_type == 'type_2':\n",
    "            letter = re.sub(u'đ', 'd', letter)\n",
    "            letter = re.sub(u'Đ', 'D', letter)\n",
    "            return ''.join(c for c in unicodedata.normalize('NFD', letter)\\\n",
    "                           if unicodedata.category(c) != 'Mn')\n",
    "        else:\n",
    "            raise ValueError('Should be: type_1 or type_2')\n",
    "\n",
    "    def get_accents(self, letter):\n",
    "        mark_accent, vowel_accent = None, None\n",
    "        bi_unicode = unicodedata.decomposition(letter).split()\n",
    "\n",
    "        if letter=='đ' or letter=='Đ':\n",
    "            mark_accent = '<9>'\n",
    "        elif bi_unicode[1]=='0302' or (bi_unicode[0] in self.circumflex_unicodes) or letter=='ậ' or letter=='Ậ':\n",
    "            mark_accent = '<6>' # VNI '<CIRCUMFLEX>'\n",
    "        elif bi_unicode[1]=='0306' or (bi_unicode[0] in self.breve_unicodes) or letter=='ặ' or letter=='Ặ':\n",
    "            mark_accent = '<8>' # '<BREVE>'\n",
    "        elif bi_unicode[1]=='031B' or (bi_unicode[0] in self._7_unicodes):\n",
    "            mark_accent = '<7>'\n",
    "            \n",
    "        if letter=='đ' or letter=='Đ':\n",
    "            vowel_accent = None\n",
    "        elif bi_unicode[1]=='0300':\n",
    "            vowel_accent = '<F>'\n",
    "        elif bi_unicode[1]=='0301':\n",
    "            vowel_accent = '<S>'\n",
    "        elif bi_unicode[1]=='0303':\n",
    "            vowel_accent = '<X>'\n",
    "        elif bi_unicode[1]=='0309':\n",
    "            vowel_accent = '<R>'\n",
    "        elif bi_unicode[1]=='0323' or (bi_unicode[0] in self.underdot_unicodes):\n",
    "            vowel_accent = '<J>'\n",
    "\n",
    "        return mark_accent, vowel_accent\n",
    "\n",
    "    def flatten_letter(self, letter):\n",
    "        if letter not in self.accent_letters:\n",
    "            return letter, None, None\n",
    "        unaccent_letter = self.get_unaccent(letter)\n",
    "        mark_accent, vowel_accent = self.get_accents(letter)\n",
    "        return unaccent_letter, mark_accent, vowel_accent\n",
    "    \n",
    "    def flatten_word(self, word):\n",
    "        flattened_word = ''\n",
    "        for letter in word:\n",
    "            unaccent_letter, mark_accent, vowel_accent = self.flatten_letter(letter)\n",
    "            flattened_word = flattened_word + unaccent_letter + (mark_accent if mark_accent!= None else '')\\\n",
    "            + (vowel_accent if vowel_accent!= None else '')\n",
    "        return flattened_word\n",
    "    \n",
    "    def invert(self, flattened_word):\n",
    "        accent_word = ''\n",
    "        letters = re.findall(r'[\\w]|<.*?>', flattened_word)\n",
    "        for letter in letters:\n",
    "            if len(letter) == 1:\n",
    "                accent_word = accent_word + letter\n",
    "            else: # accent\n",
    "                if letter == '<9>':\n",
    "                    accent_word = accent_word[:-1] + ('đ' if accent_word=='d' else 'Đ')\n",
    "                else:\n",
    "                    accent_word = accent_word + self.accent2unicode[letter]\n",
    "                accent_word = unicodedata.normalize('NFC', accent_word)\n",
    "        return accent_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quý - quy<S> - quý\n",
      "hóa - ho<S>a - hóa\n",
      "hoàn - hoa<F>n - hoàn\n",
      "khoáng - khoa<S>ng - khoáng\n",
      "gì - gi<F> - gì\n",
      "gìn - gi<F>n - gìn\n",
      "đoán - d<9>oa<S>n - đoán\n",
      "đứng - d<9>u<7><S>ng - đứng\n",
      "lặng - la<8><J>ng - lặng\n",
      "HĐND - HD<9>ND - HĐND\n",
      "\n",
      "\n",
      "quý hóa hoàn khoáng gì gìn đoán đứng lặng HĐND\n",
      "quý hóa hoàn khoáng gì gìn đoán đứng lặng HĐND\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattening = Flattening(f_type='type_2')\n",
    "\n",
    "# string1 = 'Chống dịch như chống giặc - Cả nước cùng chung tay'\n",
    "string1 = 'quý hóa hoàn khoáng gì gìn đoán đứng lặng HĐND'\n",
    "string2 = []\n",
    "for word in string1.split():\n",
    "    word = re.findall(r'\\w+', word)[0]\n",
    "    flattened_word = flattening.flatten_word(word)\n",
    "    accent_word = flattening.invert(flattened_word)\n",
    "    print(word, '-', flattened_word, '-', accent_word)\n",
    "    string2.append(accent_word)\n",
    "string2 = ' '.join(string2)\n",
    "\n",
    "print('\\n')\n",
    "print(string1)\n",
    "print(string2)\n",
    "string1==string2 # punctuation->false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/VNOnDB/all_word.csv', sep='\\t')\n",
    "ground_truth = df.loc[:, 'label'].astype(str)\n",
    "accent_words = []\n",
    "for word in ground_truth:\n",
    "    flattened_word = flattening.flatten_word(word)\n",
    "    accent_word = flattening.invert(flattened_word)\n",
    "    accent_words.append(accent_word)\n",
    "    if word!=accent_word:\n",
    "        print(word, '-', accent_word)\n",
    "sum(ground_truth==accent_words)==len(ground_truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
