{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flattening():\n",
    "    '''\n",
    "    f_type: flatening type\n",
    "        + type_1: ă, â, ê, ...\n",
    "        + type_2: type_1 + {đ, ơ, ư}\n",
    "    '''\n",
    "    def __init__(self, f_type='type_1'):\n",
    "        self.f_type = f_type\n",
    "        self.accent2unicode = {'<6>': '\\u0302', '<7>': '\\u031B', '<8>': '\\u0306', '<F>': '\\u0300', \\\n",
    "                               '<S>': '\\u0301', '<R>': '\\u0309', '<X>': '\\u0303', '<J>': '\\u0323'}\n",
    "        self.circumflex_unicodes = ['00C2', '00E2', '00CA', '00EA', '00D4', '00F4'] # â, Â, Ê, ...\n",
    "        self.breve_unicodes = ['0102', '0103'] # ă, Ă\n",
    "        self.underdot_unicodes = ['1EA0', '1EA1', '1EB8', '1EB9', '1ECC', '1ECD']\n",
    "        self._7_unicodes = ['01A0', '01A1', '01AF', '01B0']\n",
    "        self.accent_letters = 'À Á Ả Ã Ạ Â Ầ Ấ Ẩ Ẫ Ậ Ă Ằ Ắ Ẳ Ẵ Ặ à á ả ã ạ â ầ ấ ẩ ẫ ậ ă ằ ắ ẳ ẵ ặ\\\n",
    "        È É Ẻ Ẽ Ẹ Ê Ề Ế Ể Ễ Ệ è é ẻ ẽ ẹ ê ề ế ể ễ ệ\\\n",
    "        Ì Í Ỉ Ĩ Ị ì í ỉ ĩ ị\\\n",
    "        Ò Ó Ỏ Õ Ọ Ô Ồ Ố Ổ Ỗ Ộ Ơ Ờ Ớ Ở Ỡ Ợ ò ó ỏ õ ọ ô ồ ố ổ ỗ ộ ơ ờ ớ ở ỡ ợ\\\n",
    "        Ù Ú Ủ Ũ Ụ Ư Ừ Ứ Ử Ữ Ự ù ú ủ ũ ụ ư ừ ứ ử ữ ự\\\n",
    "        Ỳ Ý Ỷ Ỹ Ỵ ỳ ý ỷ ỹ ỵ'\n",
    "        if self.f_type=='type_2':\n",
    "            self.accent_letters += ' đ Đ'\n",
    "        self.accent_letters = self.accent_letters.split()\n",
    "    \n",
    "    def get_unaccent(self, letter):\n",
    "        letter = letter.encode('utf-8').decode('utf-8')\n",
    "        if self.f_type == 'type_1':\n",
    "            letter = re.sub(u'[àáảãạâầấẩẫậăằắẳẵặ]', 'a', letter)\n",
    "            letter = re.sub(u'[ÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶ]', 'A', letter)\n",
    "            letter = re.sub(u'[èéẹẻẽêềếệểễ]', 'e', letter)\n",
    "            letter = re.sub(u'[ÈÉẸẺẼÊỀẾỆỂỄ]', 'E', letter)\n",
    "            letter = re.sub(u'[òóọỏõôồốộổỗ]', 'o', letter)\n",
    "            letter = re.sub(u'[ÒÓỌỎÕÔỒỐỘỔỖ]', 'O', letter)\n",
    "            letter = re.sub(u'[ơờớợởỡ]', 'ơ', letter)\n",
    "            letter = re.sub(u'[ƠỜỚỢỞỠ]', 'Ơ', letter)\n",
    "            letter = re.sub(u'[ìíịỉĩ]', 'i', letter)\n",
    "            letter = re.sub(u'[ÌÍỊỈĨ]', 'I', letter)\n",
    "            letter = re.sub(u'[ùúụủũ]', 'u', letter)\n",
    "            letter = re.sub(u'[ÙÚỤỦŨ]', 'U', letter)\n",
    "            letter = re.sub(u'[ưừứựửữ]', 'ư', letter)\n",
    "            letter = re.sub(u'[ƯỪỨỰỬỮ]', 'Ư', letter)\n",
    "            letter = re.sub(u'[ỳýỵỷỹ]', 'y', letter)\n",
    "            letter = re.sub(u'[ỲÝỴỶỸ]', 'Y', letter)\n",
    "            return letter\n",
    "        elif self.f_type == 'type_2':\n",
    "            letter = re.sub(u'đ', 'd', letter)\n",
    "            letter = re.sub(u'Đ', 'D', letter)\n",
    "            return ''.join(c for c in unicodedata.normalize('NFD', letter)\\\n",
    "                           if unicodedata.category(c) != 'Mn')\n",
    "        else:\n",
    "            raise ValueError('Should be: type_1 or type_2')\n",
    "\n",
    "    def get_accents(self, letter):\n",
    "        mark_accent, vowel_accent = None, None\n",
    "        bi_unicode = unicodedata.decomposition(letter).split()\n",
    "\n",
    "        if letter=='đ' or letter=='Đ':\n",
    "            mark_accent = '<9>'\n",
    "        elif bi_unicode[1]=='0302' or (bi_unicode[0] in self.circumflex_unicodes) or letter=='ậ' or letter=='Ậ':\n",
    "            mark_accent = '<6>' # VNI '<CIRCUMFLEX>'\n",
    "        elif bi_unicode[1]=='0306' or (bi_unicode[0] in self.breve_unicodes) or letter=='ặ' or letter=='Ặ':\n",
    "            mark_accent = '<8>' # '<BREVE>'\n",
    "        elif (self.f_type=='type_2') and (bi_unicode[1]=='031B' or (bi_unicode[0] in self._7_unicodes)):\n",
    "            mark_accent = '<7>'\n",
    "            \n",
    "        if letter=='đ' or letter=='Đ':\n",
    "            vowel_accent = None\n",
    "        elif bi_unicode[1]=='0300':\n",
    "            vowel_accent = '<F>'\n",
    "        elif bi_unicode[1]=='0301':\n",
    "            vowel_accent = '<S>'\n",
    "        elif bi_unicode[1]=='0303':\n",
    "            vowel_accent = '<X>'\n",
    "        elif bi_unicode[1]=='0309':\n",
    "            vowel_accent = '<R>'\n",
    "        elif bi_unicode[1]=='0323' or (bi_unicode[0] in self.underdot_unicodes):\n",
    "            vowel_accent = '<J>'\n",
    "\n",
    "        return mark_accent, vowel_accent\n",
    "\n",
    "    def flatten_letter(self, letter):\n",
    "        if letter not in self.accent_letters:\n",
    "            return letter, None, None\n",
    "        unaccent_letter = self.get_unaccent(letter)\n",
    "        mark_accent, vowel_accent = self.get_accents(letter)\n",
    "        return unaccent_letter, mark_accent, vowel_accent\n",
    "    \n",
    "    def flatten_word(self, word):\n",
    "        flattened_word = ''\n",
    "        for letter in word:\n",
    "            unaccent_letter, mark_accent, vowel_accent = self.flatten_letter(letter)\n",
    "            flattened_word = flattened_word + unaccent_letter + (mark_accent if mark_accent!= None else '')\\\n",
    "            + (vowel_accent if vowel_accent!= None else '')\n",
    "        return flattened_word\n",
    "    \n",
    "    def invert(self, flattened_word):\n",
    "        accent_word = ''\n",
    "        letters = re.findall(r'[\\w]|<.*?>', flattened_word)\n",
    "        for letter in letters:\n",
    "            if len(letter) == 1:\n",
    "                accent_word = accent_word + letter\n",
    "            else: # accent\n",
    "                if letter == '<9>':\n",
    "                    accent_word = accent_word[:-1] + ('đ' if accent_word=='d' else 'Đ')\n",
    "                else:\n",
    "                    accent_word = accent_word + self.accent2unicode[letter]\n",
    "                accent_word = unicodedata.normalize('NFC', accent_word)\n",
    "        return accent_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# type_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quý - quy<S> - quý\n",
      "hóa - ho<S>a - hóa\n",
      "hoàn - hoa<F>n - hoàn\n",
      "khoáng - khoa<S>ng - khoáng\n",
      "gì - gi<F> - gì\n",
      "gìn - gi<F>n - gìn\n",
      "đoán - đoa<S>n - đoán\n",
      "đứng - đư<S>ng - đứng\n",
      "lặng - la<8><J>ng - lặng\n",
      "HĐND - HĐND - HĐND\n",
      "ơn - ơn - ơn\n",
      "\n",
      "\n",
      "quý hóa hoàn khoáng gì gìn đoán đứng lặng HĐND ơn\n",
      "quý hóa hoàn khoáng gì gìn đoán đứng lặng HĐND ơn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattening = Flattening(f_type='type_1')\n",
    "\n",
    "string1 = 'quý hóa hoàn khoáng gì gìn đoán đứng lặng HĐND ơn'\n",
    "string2 = []\n",
    "for word in string1.split():\n",
    "    word = re.findall(r'\\w+', word)[0]\n",
    "    flattened_word = flattening.flatten_word(word)\n",
    "    accent_word = flattening.invert(flattened_word)\n",
    "    print(f'{word: <{10}} - {flattened_word: <{20}} - {accent_word: <{10}}')\n",
    "    string2.append(accent_word)\n",
    "string2 = ' '.join(string2)\n",
    "\n",
    "print('\\n')\n",
    "print(string1)\n",
    "print(string2)\n",
    "string1==string2 # punctuation->false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ba<R>n\n",
      "cha<6><S>t\n",
      "chie<6><S>n\n",
      "nghie<6><J>p\n",
      "Bangladesh\n",
      "Environment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/VNOnDB/all_word.csv', sep='\\t')\n",
    "ground_truth = df.loc[:, 'label'].astype(str)\n",
    "accent_words = []\n",
    "max_len_flattened = 0\n",
    "for word in ground_truth:\n",
    "    flattened_word = flattening.flatten_word(word)\n",
    "    if len(re.findall(r'[\\w]|<.*?>', flattened_word))>max_len_flattened:\n",
    "        max_len_flattened = len(re.findall(r'[\\w]|<.*?>', flattened_word))\n",
    "        print(flattened_word)\n",
    "    accent_word = flattening.invert(flattened_word)\n",
    "    accent_words.append(accent_word)\n",
    "    if word!=accent_word:\n",
    "        print(word, '-', accent_word)\n",
    "sum(ground_truth==accent_words)==len(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# type_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quý        - quy<S>               - quý       \n",
      "hóa        - ho<S>a               - hóa       \n",
      "hoàn       - hoa<F>n              - hoàn      \n",
      "khoáng     - khoa<S>ng            - khoáng    \n",
      "gì         - gi<F>                - gì        \n",
      "gìn        - gi<F>n               - gìn       \n",
      "đoán       - d<9>oa<S>n           - đoán      \n",
      "đứng       - d<9>u<7><S>ng        - đứng      \n",
      "lặng       - la<8><J>ng           - lặng      \n",
      "HĐND       - HD<9>ND              - HĐND      \n",
      "\n",
      "\n",
      "quý hóa hoàn khoáng gì gìn đoán đứng lặng HĐND\n",
      "quý hóa hoàn khoáng gì gìn đoán đứng lặng HĐND\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattening = Flattening(f_type='type_2')\n",
    "\n",
    "# string1 = 'Chống dịch như chống giặc - Cả nước cùng chung tay'\n",
    "string1 = 'quý hóa hoàn khoáng gì gìn đoán đứng lặng HĐND'\n",
    "string2 = []\n",
    "for word in string1.split():\n",
    "    word = re.findall(r'\\w+', word)[0]\n",
    "    flattened_word = flattening.flatten_word(word)\n",
    "    accent_word = flattening.invert(flattened_word)\n",
    "    print(f'{word: <{10}} - {flattened_word: <{20}} - {accent_word: <{10}}')\n",
    "    string2.append(accent_word)\n",
    "string2 = ' '.join(string2)\n",
    "\n",
    "print('\\n')\n",
    "print(string1)\n",
    "print(string2)\n",
    "string1==string2 # punctuation->false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ba<R>n\n",
      "cha<6><S>t\n",
      "d<9>uo<6><R>i\n",
      "d<9>u<7>o<7><J>c\n",
      "tru<7>o<7><F>ng\n",
      "Bangladesh\n",
      "Environment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/VNOnDB/all_word.csv', sep='\\t')\n",
    "ground_truth = df.loc[:, 'label'].astype(str)\n",
    "accent_words = []\n",
    "max_len_flattened = 0\n",
    "for word in ground_truth:\n",
    "    flattened_word = flattening.flatten_word(word)\n",
    "    if len(re.findall(r'[\\w]|<.*?>', flattened_word))>max_len_flattened:\n",
    "        max_len_flattened = len(re.findall(r'[\\w]|<.*?>', flattened_word))\n",
    "        print(flattened_word)\n",
    "    accent_word = flattening.invert(flattened_word)\n",
    "    accent_words.append(accent_word)\n",
    "    if word!=accent_word:\n",
    "        print(word, '-', accent_word)\n",
    "sum(ground_truth==accent_words)==len(ground_truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
